hi everyone,

this week we will start with the long range forces part of the 
course. 

In this first video I'll show you the Barnes-Hut method. 1 way
to deal with long range forces.

The second video will show you how to compute multipole expansion
in preparation for next week when we'll look at the fast multipole
method. 

So perhaps you've already spotted that the slides are in a different
style. This week I wanted to try something new. If you click 
on the slides link in moodle to follow along you'll find that
these new slides are interactive. 

So: Barnes-Hut

This algorithm was proposed as the name implies by Barnes and Hut
in 1986. It provides a way of computing the long range forces
in nlogn complexity instead of the n squared of the naive implementation


Last week we had a look at forces that decay very quickly with
distance. For those it is easy to see that cutting them
off will yield a reasonable approximation of the true forces.
In fact we can cut off the dense force matrix and turn it into
a sparse matrix reducing the complexity to o of n

However, long range forces like gravity and the electromagnetic force
decay too slowly for this to be possible

Is it possible to reduce the complexity for the computation of 
long-range forces?

You already saw on the last slide what the answer will be:
Barnes-Hut  gives us a nlogn algorithm for the computation.
But even that can be improved. Next we will see that fast multipole
can reduce the complexity back to a linear complexity. So in other
words we can compute the N squared interactions with on the order
of n computations.

To do that we use an idea that crops up all over scientific computing:
exploiting hierarchies.

The Barnes-hut algorithm was initially created for astrophysics, so in
other words they considered gravity and they wanted to compute
the interactions of entire galaxies.

In that case n squared is an infeasible amount of computations.

but even with a long-range force the force with respect to something
with a reasonably small mass such as a star might be neglected.
the only problem is when you've got an entire galaxy full of stars
the sum of their masses becomes too large and you can't neglect them
anymore.

So the basic idea behind barnes-hut is to sum up all the masses
of stars or galaxies into bigger clusters and then treat them
as one "particle" so to speak.

The mass of the cluster is then just the sum of the masses it contains
and its location is the center of mass.

Now the hierarchies come into play.
We want to be able to construct clusters of various sizes, so that
depending on the distance from the particle we are currently
considering we can compute the forces.
To do this we subdivide the domain into a quadtree in 2 dimensions
or into an octree in 3 dimensions for each particle.

Let me quickly show you how to do that on the board.

---

interlude


---

Now we come to the actual algorithm.
Here you see some random particles.
You can pull them around a little and change where they are if you
want.
Credit for the visualisation goes to Jeffrey Heers and if you want
you can go to his website to see the animation too.

Now lets divide the domain up into a quadtree as I showed you earlier
If you add the first point the tree consists only of one node.
When you add the second point you need to subdivide the tree again
and again until it is alone in its cell
The we can slowly continue adding particles until the entire
tree has been built up.

So now we have subdivided the entire domain into a grid and we
have easy access to the hierarchy.

So next lets compute the accumulated masses and the centers
of mass.

We can see that starting from the lowest level the four cells
making up the level above get accumulated here.

lets watch it again just for fun.


