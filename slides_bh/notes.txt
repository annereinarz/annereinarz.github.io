hi everyone,

this week we will start with the long range forces part of the 
course. 

In this first video I'll show you the Barnes-Hut method. 1 way
to deal with long range forces.

The second video will show you how to compute multipole expansion
in preparation for next week when we'll look at the fast multipole
method. 

So perhaps you've already spotted that the slides are in a different
style. This week I wanted to try something new. If you click 
on the slides link in moodle to follow along you'll find that
these new slides are interactive. 

So: Barnes-Hut

This algorithm was proposed as the name implies by Barnes and Hut
in 1986. It provides a way of computing the long range forces
in nlogn complexity instead of the n squared of the naive implementation


Last week we had a look at forces that decay very quickly with
distance. For those it is easy to see that cutting them
off will yield a reasonable approximation of the true forces.
In fact we can cut off the dense force matrix and turn it into
a sparse matrix reducing the complexity to o of n

However, long range forces like gravity and the electromagnetic force
decay too slowly for this to be possible

Is it possible to reduce the complexity for the computation of 
long-range forces?

You already saw on the last slide what the answer will be:
Barnes-Hut  gives us a nlogn algorithm for the computation.
But even that can be improved. Next we will see that fast multipole
can reduce the complexity back to a linear complexity. So in other
words we can compute the N squared interactions with on the order
of n computations.

To do that we use an idea that crops up all over scientific computing:
exploiting hierarchies.

The Barnes-hut algorithm was initially created for astrophysics, so in
other words they considered gravity and they wanted to compute
the interactions of entire galaxies.

In that case n squared is an infeasible amount of computations.

but even with a long-range force the force with respect to something
with a reasonably small mass such as a star might be neglected.
the only problem is when you've got an entire galaxy full of stars
the sum of their masses becomes too large and you can't neglect them
anymore.

So the basic idea behind barnes-hut is to sum up all the masses
of stars or galaxies into bigger clusters and then treat them
as one "particle" so to speak.

The mass of the cluster is then just the sum of the masses it contains
and its location is the center of mass.

Now the hierarchies come into play.
We want to be able to construct clusters of various sizes, so that
depending on the distance from the particle we are currently
considering we can compute the forces.
To do this we subdivide the domain into a quadtree in 2 dimensions
or into an octree in 3 dimensions for each particle.

Let me quickly show you how to do that on the board.

---

interlude


---

Now we come to the actual algorithm.
Here you see some random particles.
You can pull them around a little and change where they are if you
want.
Credit for the visualisation goes to Jeffrey Heers and if you want
you can go to his website to see the animation too.

Now lets divide the domain up into a quadtree as I showed you earlier
If you add the first point the tree consists only of one node.
When you add the second point you need to subdivide the tree again
and again until it is alone in its cell
The we can slowly continue adding particles until the entire
tree has been built up.

So now we have subdivided the entire domain into a grid and we
have easy access to the hierarchy.

So next lets compute the accumulated masses and the centers
of mass.

We can see that starting from the lowest level the four cells
making up the level above get accumulated here.

so now for each layer of the hierarchy of cells we have one particle
in some cases an accumulated one.
Now we have to choose which level to interact with for each
force calculation.
Essentially we alway interact with a constant number of particles
at each level which is what gives this article its nlogn complexity
Lets see why

so here we have pointer which is our current location
then in dependence of a parameter theta and the distance
to the box we will decide which level to interact with.
Right now theta is 0, so we alway interact with a leaf.

The algorithm goes like this we start in the root node and
then descend until the diameter of the cell divided by the 
distance from it is less than theta
We then accumulate the resulting force to our current particle.

So the larger we choose theta the less far into the tree
we need to descend before the theta rule is satisfied.

Lets see what happens if we increase theta slightly
now some of the further away particles are treated using a cluster
See in the cell to the right 6 particles have been summed up 
into one particle located at their center of mass and
weighing as much as their sum.

lets increase theta some more

now many more levels up are allowed and we've summed up even more
particles.

We can also try to move the cursor representing our current location
around.

The link to the slides allows you to try this as well.

and that was already the full algorithm

if we want to look at it that way our algorithm is now seperating
    short and long range forces. Everything thats far away gets
    treated approximately and for short range we do the same
    as in the short-range potentials part of the lecture.
So the main difference with barnes hut is that instead of cutting
the long range interactions entirely we now approximate them.

Obviously by approximating far away forces we make an error.
same as we did with short range potentials by setting the force
to zero.
In this case the size of the error depends on theta
If theta is small we hardly group together any particles and the error
is low, but the cost is higher
and if theta is larger the cost is much lower but our accuracy is lower

Now, we have simply summed up all the particles into one point mass
this is a very low order approximation of the particles
and thus the convergence with respect to theta will be slow

In the fast multipole algorithm we will have a look at higher order
approximations for the clusters of particles.

It is clear that as theta goes to zero we get back to the naive
algorithm, ie we calculate only with the leaves of the octree.
But for a reasonable theta we get
a complexity of nlogn because the number of cells is logn divided
by theta to the power of 3 in 3D or 2 in 2D.

Thats quite intuitive since the diameter of the cells halves
with each level of the tree.

So for each of our n particle we have log n divided by theta to the power
of the dimension interactions to compute giving us an nlogn effort

So lets summarise:

we use a pseudo particles, which you can think of as clusters of 
particles approximated as a point mass.

we traverse the tree starting at the root and compute
pseudo particles for each level.

then we compute the forces by looping over all particles and for
each traversing the tree until the theta rule is satisfied.

Around all this there is a time integration, which requires
us to recompute or update our data structures in each step.

and finally: barnes hut is suitable for long-range forces,
it has complexity of nlogn and uses a low order approximation
for the pseudo particles.

    we can do better both on the complexity and on improving
    the accuracy of the pseudo particles

    so next lets look at multipole expansions.

